{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6a9be02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "import supervision as sv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "38c00484",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=\"weights/sam_vit_h_4b8939.pth\").to(DEVICE)\n",
    "predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1423738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = r\"C:\\Users\\eleut\\OneDrive\\Desktop\\Cows\\20250401_123319.mp4\"\n",
    "assert os.path.exists(VIDEO_PATH), \"Video file not found!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0758609",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE=\"vit_h\"\n",
    "CHECKPOINT_PATH=\"weights/sam_vit_h_4b8939.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44ea67ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAM loaded on cpu\n"
     ]
    }
   ],
   "source": [
    "mask_generator = SamAutomaticMaskGenerator(\n",
    "    sam,\n",
    "    points_per_side=32,\n",
    "    pred_iou_thresh=0.86,\n",
    "    stability_score_thresh=0.92,\n",
    "    min_mask_region_area=200  # Adjust based on cow size\n",
    ")\n",
    "print(f\"SAM loaded on {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5cd6d4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video info: 30.00434401267618 FPS, 9983 frames\n",
      "Extracted 333 frames from 9983 total frames\n",
      "You have 333 frames to process\n"
     ]
    }
   ],
   "source": [
    "def extract_frames(VIDEO_PATH, interval=1):\n",
    "    cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video\")\n",
    "        return [], []\n",
    "    \n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"Video info: {fps} FPS, {total_frames} frames\")\n",
    "    \n",
    "    frame_interval = max(1, int(fps * interval))\n",
    "    frames = []\n",
    "    frame_count = 0\n",
    "    extracted_count = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        if frame_count % frame_interval == 0:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames.append(frame_rgb)\n",
    "            extracted_count += 1\n",
    "            \n",
    "        frame_count += 1\n",
    "    \n",
    "    cap.release()\n",
    "    print(f\"Extracted {extracted_count} frames from {frame_count} total frames\")\n",
    "    return frames, np.arange(0, frame_count/fps, 1/interval)[:len(frames)]\n",
    "\n",
    "# Extract frames (1 frame per second)\n",
    "VIDEO_PATH = r\"C:\\Users\\eleut\\OneDrive\\Desktop\\Cows\\20250401_123319.mp4\"\n",
    "frames, times = extract_frames(VIDEO_PATH, interval=1)\n",
    "print(f\"You have {len(frames)} frames to process\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee8d701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Processing first 5 frames...\n",
      "Processing frame 1/333...\n",
      "Generated 102 masks\n",
      "Processing frame 2/333...\n",
      "Generated 103 masks\n",
      "Processing frame 3/333...\n",
      "Generated 119 masks\n",
      "Processing frame 4/333...\n",
      "Generated 99 masks\n",
      "Processing frame 5/333...\n",
      "Generated 96 masks\n"
     ]
    }
   ],
   "source": [
    "def process_frames_with_sam(frames, start_idx=0, num_frames=5):\n",
    "    results = []\n",
    "    \n",
    "    for i in range(start_idx, min(start_idx + num_frames, len(frames))):\n",
    "        print(f\"Processing frame {i+1}/{len(frames)}...\")\n",
    "        \n",
    "        # Generate masks\n",
    "        sam_result = mask_generator.generate(frames[i])\n",
    "        \n",
    "        # Store results\n",
    "        results.append({\n",
    "            'frame_idx': i,\n",
    "            'time': times[i],\n",
    "            'masks': sam_result,\n",
    "            'frame': frames[i]\n",
    "        })\n",
    "        print(f\"Generated {len(sam_result)} masks\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\" Processing first 5 frames...\")\n",
    "sample_results = process_frames_with_sam(frames, num_frames=min(5, len(frames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a928381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Visualizing first frame results...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m Visualizing first frame results...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# visualize_frame_results(sample_results[0])\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43mvisualize_frame_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Show largest mask\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# largest_mask = max(sample_results[0]['masks'], key=lambda x: x['area'])\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# largest_mask = max(sample_results['masks'], key=lambda x: x['area'])\u001b[39;00m\n\u001b[32m     42\u001b[39m plt.figure(figsize=(\u001b[32m8\u001b[39m, \u001b[32m6\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mvisualize_frame_results\u001b[39m\u001b[34m(frame_result)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvisualize_frame_results\u001b[39m(frame_result):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     frame_rgb   = \u001b[43mframe_result\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mframe\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      5\u001b[39m     sam_result  = frame_result[\u001b[33m'\u001b[39m\u001b[33mmasks\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# Convert to BGR for supervision\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "from supervision import ColorLookup\n",
    "\n",
    "def visualize_frame_results(frame_result):\n",
    "    frame_rgb   = frame_result['frame']\n",
    "    sam_result  = frame_result['masks']\n",
    "\n",
    "    # Convert to BGR for supervision\n",
    "    frame_bgr = cv2.cvtColor(frame_rgb, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Build MaskAnnotator and detections\n",
    "    mask_annotator = sv.MaskAnnotator()\n",
    "    detections     = sv.Detections.from_sam(sam_result)\n",
    "\n",
    "    # Pass the lookup via the 'custom_color_lookup' arg\n",
    "    annotated_image = mask_annotator.annotate(\n",
    "        scene=frame_bgr,\n",
    "        detections=detections,\n",
    "        custom_color_lookup=ColorLookup.INDEX\n",
    "    )\n",
    "\n",
    "    # Plot results\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    ax[0].imshow(frame_rgb)\n",
    "    ax[0].set_title(f'Original Frame {frame_result[\"frame_idx\"]}')\n",
    "    ax[0].axis('off')\n",
    "\n",
    "    ax[1].imshow(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))\n",
    "    ax[1].set_title(f'SAM Segmentation: {len(sam_result)} masks')\n",
    "    ax[1].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize first frame results\n",
    "if sample_results:\n",
    "    print(\" Visualizing first frame results...\")\n",
    "    # visualize_frame_results(sample_results[0])\n",
    "    visualize_frame_results(sample_results)\n",
    "    # Show largest mask\n",
    "    # largest_mask = max(sample_results[0]['masks'], key=lambda x: x['area'])\n",
    "    # largest_mask = max(sample_results['masks'], key=lambda x: x['area'])\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    # plt.imshow(largest_mask['segmentation'], cmap='gray')\n",
    "    # plt.title(f\"Largest Object Mask ({largest_mask['area']} pixels)\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No results to visualize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b46dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5e0dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eebc078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc2c48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
